{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./drophyper/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[K     |████████████████████████████████| 508 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in ./drophyper/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./drophyper/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/home/abazouzi/Documents/Code/DropHyper/drophyper/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dhg import Hypergraph\n",
    "from dhg.data import Cooking200\n",
    "from dhg.models import HGNN\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.40277s, Loss: 2.99680\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 0.30828s, Loss: 2.71560\n",
      "Epoch: 2, Time: 0.30597s, Loss: 2.34183\n",
      "Epoch: 3, Time: 0.30820s, Loss: 2.17803\n",
      "Epoch: 4, Time: 0.30484s, Loss: 2.04616\n",
      "Epoch: 5, Time: 0.30546s, Loss: 1.90518\n",
      "Epoch: 6, Time: 0.29929s, Loss: 1.78512\n",
      "Epoch: 7, Time: 0.30219s, Loss: 1.66366\n",
      "Epoch: 8, Time: 0.30903s, Loss: 1.53951\n",
      "Epoch: 9, Time: 0.31299s, Loss: 1.43321\n",
      "Epoch: 10, Time: 0.29602s, Loss: 1.34124\n",
      "Epoch: 11, Time: 0.31282s, Loss: 1.22620\n",
      "Epoch: 12, Time: 0.30538s, Loss: 1.11851\n",
      "Epoch: 13, Time: 0.31003s, Loss: 1.01421\n",
      "Epoch: 14, Time: 0.30352s, Loss: 0.93399\n",
      "Epoch: 15, Time: 0.31174s, Loss: 0.83967\n",
      "Epoch: 16, Time: 0.32792s, Loss: 0.76134\n",
      "Epoch: 17, Time: 0.32425s, Loss: 0.68312\n",
      "update best: 0.05500\n",
      "Epoch: 18, Time: 0.33249s, Loss: 0.61719\n",
      "update best: 0.07000\n",
      "Epoch: 19, Time: 0.33323s, Loss: 0.56950\n",
      "update best: 0.08500\n",
      "Epoch: 20, Time: 0.32288s, Loss: 0.50835\n",
      "update best: 0.09500\n",
      "Epoch: 21, Time: 0.31636s, Loss: 0.44322\n",
      "update best: 0.11000\n",
      "Epoch: 22, Time: 0.31806s, Loss: 0.39863\n",
      "update best: 0.12000\n",
      "Epoch: 23, Time: 0.32018s, Loss: 0.35737\n",
      "update best: 0.13000\n",
      "Epoch: 24, Time: 0.31626s, Loss: 0.32382\n",
      "update best: 0.13500\n",
      "Epoch: 25, Time: 0.30975s, Loss: 0.29459\n",
      "Epoch: 26, Time: 0.31738s, Loss: 0.25847\n",
      "update best: 0.14000\n",
      "Epoch: 27, Time: 0.31776s, Loss: 0.23320\n",
      "Epoch: 28, Time: 0.31746s, Loss: 0.20159\n",
      "Epoch: 29, Time: 0.31669s, Loss: 0.18012\n",
      "Epoch: 30, Time: 0.32015s, Loss: 0.16432\n",
      "Epoch: 31, Time: 0.31179s, Loss: 0.14314\n",
      "Epoch: 32, Time: 0.33000s, Loss: 0.12413\n",
      "Epoch: 33, Time: 0.30948s, Loss: 0.11918\n",
      "Epoch: 34, Time: 0.30865s, Loss: 0.10914\n",
      "update best: 0.15000\n",
      "Epoch: 35, Time: 0.31827s, Loss: 0.09783\n",
      "update best: 0.15500\n",
      "Epoch: 36, Time: 0.32348s, Loss: 0.09012\n",
      "Epoch: 37, Time: 0.32377s, Loss: 0.08263\n",
      "Epoch: 38, Time: 0.31925s, Loss: 0.07588\n",
      "Epoch: 39, Time: 0.31384s, Loss: 0.07045\n",
      "Epoch: 40, Time: 0.32379s, Loss: 0.06601\n",
      "Epoch: 41, Time: 0.31469s, Loss: 0.06401\n",
      "Epoch: 42, Time: 0.31373s, Loss: 0.05708\n",
      "Epoch: 43, Time: 0.31874s, Loss: 0.05933\n",
      "update best: 0.16500\n",
      "Epoch: 44, Time: 0.32006s, Loss: 0.05199\n",
      "Epoch: 45, Time: 0.31203s, Loss: 0.05083\n",
      "update best: 0.17000\n",
      "Epoch: 46, Time: 0.31501s, Loss: 0.05015\n",
      "Epoch: 47, Time: 0.31624s, Loss: 0.04578\n",
      "Epoch: 48, Time: 0.31261s, Loss: 0.04438\n",
      "Epoch: 49, Time: 0.31197s, Loss: 0.04232\n",
      "Epoch: 50, Time: 0.31074s, Loss: 0.04272\n",
      "Epoch: 51, Time: 0.30860s, Loss: 0.04105\n",
      "Epoch: 52, Time: 0.31224s, Loss: 0.04030\n",
      "Epoch: 53, Time: 0.30993s, Loss: 0.04078\n",
      "Epoch: 54, Time: 0.33605s, Loss: 0.04040\n",
      "update best: 0.17500\n",
      "Epoch: 55, Time: 0.32906s, Loss: 0.03875\n",
      "update best: 0.18000\n",
      "Epoch: 56, Time: 0.32821s, Loss: 0.03927\n",
      "update best: 0.18500\n",
      "Epoch: 57, Time: 0.32969s, Loss: 0.03890\n",
      "update best: 0.20000\n",
      "Epoch: 58, Time: 0.31568s, Loss: 0.03792\n",
      "update best: 0.22500\n",
      "Epoch: 59, Time: 0.29193s, Loss: 0.03601\n",
      "update best: 0.25500\n",
      "Epoch: 60, Time: 0.29562s, Loss: 0.03728\n",
      "Epoch: 61, Time: 0.31483s, Loss: 0.03460\n",
      "Epoch: 62, Time: 0.30047s, Loss: 0.03625\n",
      "Epoch: 63, Time: 0.29747s, Loss: 0.03420\n",
      "Epoch: 64, Time: 0.29443s, Loss: 0.03390\n",
      "Epoch: 65, Time: 0.29215s, Loss: 0.03164\n",
      "Epoch: 66, Time: 0.29505s, Loss: 0.03082\n",
      "Epoch: 67, Time: 0.29720s, Loss: 0.03488\n",
      "update best: 0.27000\n",
      "Epoch: 68, Time: 0.29687s, Loss: 0.03148\n",
      "update best: 0.28000\n",
      "Epoch: 69, Time: 0.29085s, Loss: 0.03049\n",
      "update best: 0.28500\n",
      "Epoch: 70, Time: 0.29853s, Loss: 0.02995\n",
      "update best: 0.30500\n",
      "Epoch: 71, Time: 0.29997s, Loss: 0.02857\n",
      "update best: 0.32500\n",
      "Epoch: 72, Time: 0.29703s, Loss: 0.02925\n",
      "update best: 0.34000\n",
      "Epoch: 73, Time: 0.29775s, Loss: 0.02820\n",
      "update best: 0.35000\n",
      "Epoch: 74, Time: 0.29767s, Loss: 0.02768\n",
      "Epoch: 75, Time: 0.29754s, Loss: 0.02722\n",
      "Epoch: 76, Time: 0.29616s, Loss: 0.02600\n",
      "update best: 0.37500\n",
      "Epoch: 77, Time: 0.29921s, Loss: 0.02712\n",
      "update best: 0.38500\n",
      "Epoch: 78, Time: 0.29571s, Loss: 0.02537\n",
      "update best: 0.39500\n",
      "Epoch: 79, Time: 0.28424s, Loss: 0.02641\n",
      "update best: 0.41000\n",
      "Epoch: 80, Time: 0.29588s, Loss: 0.02702\n",
      "Epoch: 81, Time: 0.31046s, Loss: 0.02566\n",
      "Epoch: 82, Time: 0.28910s, Loss: 0.02484\n",
      "Epoch: 83, Time: 0.29208s, Loss: 0.02565\n",
      "update best: 0.42000\n",
      "Epoch: 84, Time: 0.30249s, Loss: 0.02733\n",
      "update best: 0.43500\n",
      "Epoch: 85, Time: 0.33377s, Loss: 0.02505\n",
      "Epoch: 86, Time: 0.32785s, Loss: 0.02769\n",
      "update best: 0.45500\n",
      "Epoch: 87, Time: 0.32961s, Loss: 0.02247\n",
      "update best: 0.46500\n",
      "Epoch: 88, Time: 0.32623s, Loss: 0.02959\n",
      "Epoch: 89, Time: 0.32744s, Loss: 0.02251\n",
      "Epoch: 90, Time: 0.33405s, Loss: 0.02952\n",
      "Epoch: 91, Time: 0.32959s, Loss: 0.02501\n",
      "Epoch: 92, Time: 0.32542s, Loss: 0.02214\n",
      "Epoch: 93, Time: 0.32611s, Loss: 0.02571\n",
      "Epoch: 94, Time: 0.34552s, Loss: 0.02697\n",
      "Epoch: 95, Time: 0.34430s, Loss: 0.02109\n",
      "Epoch: 96, Time: 0.33515s, Loss: 0.02073\n",
      "Epoch: 97, Time: 0.33228s, Loss: 0.02408\n",
      "Epoch: 98, Time: 0.33391s, Loss: 0.02413\n",
      "Epoch: 99, Time: 0.33030s, Loss: 0.01946\n",
      "Epoch: 100, Time: 0.33440s, Loss: 0.01900\n",
      "Epoch: 101, Time: 0.33074s, Loss: 0.01899\n",
      "Epoch: 102, Time: 0.33093s, Loss: 0.01972\n",
      "Epoch: 103, Time: 0.33292s, Loss: 0.01847\n",
      "Epoch: 104, Time: 0.33186s, Loss: 0.01856\n",
      "Epoch: 105, Time: 0.33156s, Loss: 0.01739\n",
      "Epoch: 106, Time: 0.32790s, Loss: 0.01672\n",
      "Epoch: 107, Time: 0.32663s, Loss: 0.01692\n",
      "Epoch: 108, Time: 0.33097s, Loss: 0.01705\n",
      "Epoch: 109, Time: 0.33587s, Loss: 0.01613\n",
      "update best: 0.49500\n",
      "Epoch: 110, Time: 0.32735s, Loss: 0.01732\n",
      "Epoch: 111, Time: 0.32887s, Loss: 0.01689\n",
      "Epoch: 112, Time: 0.32873s, Loss: 0.01766\n",
      "Epoch: 113, Time: 0.33052s, Loss: 0.01626\n",
      "Epoch: 114, Time: 0.33252s, Loss: 0.01676\n",
      "Epoch: 115, Time: 0.33113s, Loss: 0.01671\n",
      "Epoch: 116, Time: 0.33143s, Loss: 0.01779\n",
      "Epoch: 117, Time: 0.33420s, Loss: 0.01616\n",
      "Epoch: 118, Time: 0.33069s, Loss: 0.01756\n",
      "Epoch: 119, Time: 0.33087s, Loss: 0.01577\n",
      "Epoch: 120, Time: 0.32807s, Loss: 0.01559\n",
      "Epoch: 121, Time: 0.33427s, Loss: 0.01742\n",
      "Epoch: 122, Time: 0.33411s, Loss: 0.01643\n",
      "Epoch: 123, Time: 0.33096s, Loss: 0.01526\n",
      "Epoch: 124, Time: 0.32892s, Loss: 0.01603\n",
      "Epoch: 125, Time: 0.32607s, Loss: 0.01626\n",
      "Epoch: 126, Time: 0.32776s, Loss: 0.01482\n",
      "Epoch: 127, Time: 0.33661s, Loss: 0.01500\n",
      "Epoch: 128, Time: 0.32541s, Loss: 0.01774\n",
      "Epoch: 129, Time: 0.32989s, Loss: 0.01658\n",
      "Epoch: 130, Time: 0.32810s, Loss: 0.01855\n",
      "Epoch: 131, Time: 0.33254s, Loss: 0.01435\n",
      "Epoch: 132, Time: 0.32935s, Loss: 0.01664\n",
      "Epoch: 133, Time: 0.33538s, Loss: 0.01646\n",
      "Epoch: 134, Time: 0.32895s, Loss: 0.01340\n",
      "Epoch: 135, Time: 0.33094s, Loss: 0.01593\n",
      "Epoch: 136, Time: 0.32950s, Loss: 0.01721\n",
      "Epoch: 137, Time: 0.32730s, Loss: 0.01432\n",
      "Epoch: 138, Time: 0.35713s, Loss: 0.01615\n",
      "Epoch: 139, Time: 0.35055s, Loss: 0.01693\n",
      "Epoch: 140, Time: 0.32643s, Loss: 0.01587\n",
      "Epoch: 141, Time: 0.32924s, Loss: 0.01346\n",
      "Epoch: 142, Time: 0.33628s, Loss: 0.01492\n",
      "Epoch: 143, Time: 0.32656s, Loss: 0.01444\n",
      "Epoch: 144, Time: 0.32057s, Loss: 0.01308\n",
      "Epoch: 145, Time: 0.32447s, Loss: 0.01286\n",
      "Epoch: 146, Time: 0.31450s, Loss: 0.01407\n",
      "Epoch: 147, Time: 0.31989s, Loss: 0.01367\n",
      "Epoch: 148, Time: 0.32144s, Loss: 0.01531\n",
      "Epoch: 149, Time: 0.32104s, Loss: 0.01166\n",
      "Epoch: 150, Time: 0.32285s, Loss: 0.01362\n",
      "Epoch: 151, Time: 0.32444s, Loss: 0.01456\n",
      "Epoch: 152, Time: 0.32684s, Loss: 0.01250\n",
      "Epoch: 153, Time: 0.32342s, Loss: 0.01279\n",
      "Epoch: 154, Time: 0.32440s, Loss: 0.01400\n",
      "Epoch: 155, Time: 0.32178s, Loss: 0.01363\n",
      "Epoch: 156, Time: 0.32910s, Loss: 0.01219\n",
      "Epoch: 157, Time: 0.31763s, Loss: 0.01558\n",
      "Epoch: 158, Time: 0.31809s, Loss: 0.01594\n",
      "Epoch: 159, Time: 0.31946s, Loss: 0.01395\n",
      "Epoch: 160, Time: 0.32644s, Loss: 0.01408\n",
      "Epoch: 161, Time: 0.32267s, Loss: 0.01366\n",
      "Epoch: 162, Time: 0.32155s, Loss: 0.01372\n",
      "Epoch: 163, Time: 0.31222s, Loss: 0.01449\n",
      "Epoch: 164, Time: 0.33296s, Loss: 0.01306\n",
      "Epoch: 165, Time: 0.33652s, Loss: 0.01406\n",
      "Epoch: 166, Time: 0.34370s, Loss: 0.01461\n",
      "Epoch: 167, Time: 0.33672s, Loss: 0.01281\n",
      "Epoch: 168, Time: 0.32385s, Loss: 0.01486\n",
      "Epoch: 169, Time: 0.33641s, Loss: 0.01230\n",
      "Epoch: 170, Time: 0.32499s, Loss: 0.01490\n",
      "Epoch: 171, Time: 0.32591s, Loss: 0.01257\n",
      "Epoch: 172, Time: 0.33343s, Loss: 0.01321\n",
      "Epoch: 173, Time: 0.32601s, Loss: 0.01245\n",
      "Epoch: 174, Time: 0.33443s, Loss: 0.01232\n",
      "Epoch: 175, Time: 0.36778s, Loss: 0.01213\n",
      "Epoch: 176, Time: 0.33047s, Loss: 0.01227\n",
      "Epoch: 177, Time: 0.33282s, Loss: 0.01216\n",
      "Epoch: 178, Time: 0.32667s, Loss: 0.01190\n",
      "Epoch: 179, Time: 0.32458s, Loss: 0.01130\n",
      "Epoch: 180, Time: 0.33107s, Loss: 0.01148\n",
      "Epoch: 181, Time: 0.33086s, Loss: 0.01180\n",
      "Epoch: 182, Time: 0.32281s, Loss: 0.01256\n",
      "update best: 0.50500\n",
      "Epoch: 183, Time: 0.31919s, Loss: 0.01114\n",
      "Epoch: 184, Time: 0.33186s, Loss: 0.01144\n",
      "Epoch: 185, Time: 0.33822s, Loss: 0.01117\n",
      "Epoch: 186, Time: 0.32536s, Loss: 0.01066\n",
      "Epoch: 187, Time: 0.31950s, Loss: 0.01114\n",
      "Epoch: 188, Time: 0.33193s, Loss: 0.01064\n",
      "Epoch: 189, Time: 0.33444s, Loss: 0.01112\n",
      "Epoch: 190, Time: 0.33428s, Loss: 0.01076\n",
      "Epoch: 191, Time: 0.33827s, Loss: 0.01163\n",
      "Epoch: 192, Time: 0.32940s, Loss: 0.01142\n",
      "Epoch: 193, Time: 0.32735s, Loss: 0.01277\n",
      "Epoch: 194, Time: 0.33221s, Loss: 0.01070\n",
      "Epoch: 195, Time: 0.33438s, Loss: 0.01071\n",
      "Epoch: 196, Time: 0.32186s, Loss: 0.01241\n",
      "Epoch: 197, Time: 0.35102s, Loss: 0.01101\n",
      "Epoch: 198, Time: 0.32963s, Loss: 0.01160\n",
      "Epoch: 199, Time: 0.32927s, Loss: 0.01096\n",
      "\n",
      "train finished!\n",
      "best val: 0.50500\n",
      "test...\n",
      "final result: epoch: 182\n",
      "{'accuracy': 0.5244895219802856, 'f1_score': 0.3968643277807894, 'f1_score -> average@micro': 0.5244895044980723}\n"
     ]
    }
   ],
   "source": [
    "set_seed(2021)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "data = Cooking200()\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drophyper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
