{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dhg import Hypergraph\n",
    "from dhg.data import Cooking200, CocitationCora, CoauthorshipCora, DBLP4k, Tencent2k\n",
    "from dhg.models import HGNN, HGNNP, HyperGCN, UniSAGE\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.03744s, Loss: 1.47682\n",
      "update best: 0.62500\n",
      "Epoch: 1, Time: 0.01640s, Loss: 2.64121\n",
      "Epoch: 2, Time: 0.01576s, Loss: 0.46155\n",
      "Epoch: 3, Time: 0.01656s, Loss: 0.48771\n",
      "Epoch: 4, Time: 0.01648s, Loss: 0.34893\n",
      "Epoch: 5, Time: 0.01587s, Loss: 0.18031\n",
      "Epoch: 6, Time: 0.01717s, Loss: 0.18295\n",
      "Epoch: 7, Time: 0.01643s, Loss: 0.14636\n",
      "Epoch: 8, Time: 0.01571s, Loss: 0.13556\n",
      "Epoch: 9, Time: 0.01619s, Loss: 0.09177\n",
      "Epoch: 10, Time: 0.01752s, Loss: 0.11499\n",
      "Epoch: 11, Time: 0.01610s, Loss: 0.12797\n",
      "Epoch: 12, Time: 0.01608s, Loss: 0.11654\n",
      "Epoch: 13, Time: 0.01591s, Loss: 0.05929\n",
      "Epoch: 14, Time: 0.01639s, Loss: 0.08277\n",
      "Epoch: 15, Time: 0.01693s, Loss: 0.05057\n",
      "Epoch: 16, Time: 0.01585s, Loss: 0.06059\n",
      "Epoch: 17, Time: 0.01719s, Loss: 0.06320\n",
      "update best: 0.67500\n",
      "Epoch: 18, Time: 0.01746s, Loss: 0.04078\n",
      "update best: 0.75000\n",
      "Epoch: 19, Time: 0.01689s, Loss: 0.06295\n",
      "update best: 0.77500\n",
      "Epoch: 20, Time: 0.01656s, Loss: 0.01748\n",
      "update best: 0.85000\n",
      "Epoch: 21, Time: 0.01724s, Loss: 0.01381\n",
      "Epoch: 22, Time: 0.01611s, Loss: 0.03243\n",
      "Epoch: 23, Time: 0.01608s, Loss: 0.02340\n",
      "Epoch: 24, Time: 0.01760s, Loss: 0.00541\n",
      "Epoch: 25, Time: 0.01712s, Loss: 0.00312\n",
      "Epoch: 26, Time: 0.01784s, Loss: 0.01599\n",
      "Epoch: 27, Time: 0.01778s, Loss: 0.08186\n",
      "Epoch: 28, Time: 0.01755s, Loss: 0.03751\n",
      "Epoch: 29, Time: 0.01742s, Loss: 0.00728\n",
      "Epoch: 30, Time: 0.01678s, Loss: 0.00408\n",
      "Epoch: 31, Time: 0.01754s, Loss: 0.00372\n",
      "Epoch: 32, Time: 0.01636s, Loss: 0.00309\n",
      "Epoch: 33, Time: 0.01752s, Loss: 0.01126\n",
      "Epoch: 34, Time: 0.01707s, Loss: 0.03027\n",
      "Epoch: 35, Time: 0.01638s, Loss: 0.00485\n",
      "Epoch: 36, Time: 0.02364s, Loss: 0.00266\n",
      "Epoch: 37, Time: 0.02291s, Loss: 0.01278\n",
      "Epoch: 38, Time: 0.02286s, Loss: 0.01290\n",
      "Epoch: 39, Time: 0.02252s, Loss: 0.00889\n",
      "Epoch: 40, Time: 0.02253s, Loss: 0.00520\n",
      "Epoch: 41, Time: 0.02196s, Loss: 0.02058\n",
      "Epoch: 42, Time: 0.02220s, Loss: 0.00424\n",
      "Epoch: 43, Time: 0.02184s, Loss: 0.00316\n",
      "Epoch: 44, Time: 0.02190s, Loss: 0.00512\n",
      "Epoch: 45, Time: 0.02232s, Loss: 0.00044\n",
      "Epoch: 46, Time: 0.02378s, Loss: 0.00105\n",
      "Epoch: 47, Time: 0.02290s, Loss: 0.00077\n",
      "Epoch: 48, Time: 0.02296s, Loss: 0.00297\n",
      "Epoch: 49, Time: 0.02206s, Loss: 0.00090\n",
      "\n",
      "train finished!\n",
      "best val: 0.85000\n",
      "test...\n",
      "final result: epoch: 20\n",
      "{'accuracy': 0.6687440276145935, 'f1_score': 0.5132545743418131, 'f1_score -> average@micro': 0.6687440076701822}\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "data = Tencent2k()\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = UniSAGE(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(50):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drophyper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
